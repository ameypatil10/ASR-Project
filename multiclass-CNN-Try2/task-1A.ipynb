{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class Hparams():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "        \"\"\"\n",
    "        Data Parameters\n",
    "        \"\"\"\n",
    "\n",
    "        # os.makedirs('../input', exist_ok=True)\n",
    "        os.makedirs('../model', exist_ok=True)\n",
    "        # os.makedirs('../data/', exist_ok=True)\n",
    "        os.makedirs('../results/', exist_ok=True)\n",
    "\n",
    "        self.train_csv = '/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-development/evaluation_setup/train.csv'\n",
    "        self.valid_csv = '/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-development/evaluation_setup/valid.csv'\n",
    "#         self.submit_csv = '../input/task1asubmitcsv/test.csv'\n",
    "        self.submit_csv = '/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-leaderboard/evaluation_setup/test.csv'\n",
    "\n",
    "        self.dev_file = '../features/logmel_64frames_64melbins/TAU-urban-acoustic-scenes-2019-development.h5'\n",
    "#         self.submit_file = '../input/task1aleaderboardh5/TAU-urban-acoustic-scenes-2019-leaderboard.h5'\n",
    "        self.submit_file = '../features/task1aleaderboardh5/TAU-urban-acoustic-scenes-2019-leaderboard.h5'\n",
    "\n",
    "        \"\"\"\n",
    "        Model Parameters\n",
    "        \"\"\"\n",
    "\n",
    "        os.makedirs('../model/', exist_ok=True)\n",
    "\n",
    "        self.input_shape = (640, 64)\n",
    "        self.num_channel = 64\n",
    "        self.num_classes = 10\n",
    "\n",
    "        self.id_to_class = {\n",
    "            0: 'airport',\n",
    "            1: 'shopping_mall',\n",
    "            2: 'metro_station',\n",
    "            3: 'street_pedestrian',\n",
    "            4: 'public_square',\n",
    "            5: 'street_traffic',\n",
    "            6: 'tram',\n",
    "            7: 'bus',\n",
    "            8: 'metro',\n",
    "            9: 'park',\n",
    "        }\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Training parameters\n",
    "        \"\"\"\n",
    "\n",
    "        self.gpu_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device_ids = [0,1]\n",
    "\n",
    "        self.pretrained = False\n",
    "\n",
    "        self.thresh = 0.5\n",
    "        self.repeat_infer = 1\n",
    "\n",
    "        self.num_epochs = 200\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.learning_rate = 0.00001\n",
    "\n",
    "        self.momentum1 = 0.5\n",
    "        self.momentum2 = 0.999\n",
    "\n",
    "        self.avg_mode = 'micro'\n",
    "\n",
    "        self.print_interval = 1000\n",
    "\n",
    "        ################################################################################################################################################\n",
    "        self.exp_name = 'multiclass-CNN/'\n",
    "        ################################################################################################################################################\n",
    "\n",
    "        self.result_dir = '../results/'+self.exp_name\n",
    "        os.makedirs(self.result_dir, exist_ok=True)\n",
    "\n",
    "        self.model_dir = '../model/' + self.exp_name\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "        self.model = self.model_dir + 'model'\n",
    "\n",
    "\n",
    "hparams = Hparams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # self.model = resnext101_64x4d(num_classes=1000, pretrained='imagenet')\n",
    "        # num_ftrs = self.model.last_linear.in_features\n",
    "        # self.model.last_linear = nn.Sequential(\n",
    "        #                             nn.Linear(num_ftrs, hparams.num_classes),\n",
    "        #                             nn.Sigmoid())\n",
    "        self.model = models.densenet121(pretrained=False, progress=True)\n",
    "        num_ftrs = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "                                    nn.Linear(num_ftrs, hparams.num_classes),\n",
    "                                    nn.Softmax(dim=1))\n",
    "\n",
    "        # self.model = models.squeezenet1_0(pretrained=True, progress=True)\n",
    "        # self.model.classifier[1] = nn.Conv2d(512, 1, kernel_size=(1,1), stride=(1,1))\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import h5py\n",
    "import random\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "\n",
    "import code\n",
    "\n",
    "class AudioData(Dataset):\n",
    "\n",
    "  def __init__(self, data_csv, data_file, transform=None, input_shape=hparams.input_shape, pre_process=None, ds_type=''):\n",
    "        'Initialization'\n",
    "        self.data_csv = data_csv\n",
    "        self.data_file = data_file\n",
    "        self.input_shape = hparams.input_shape\n",
    "        self.ds_type = ds_type\n",
    "        self.transform = transform\n",
    "        self.pre_process = pre_process\n",
    "        self.data_frame = pd.read_csv(data_csv)\n",
    "        data = h5py.File(data_file)\n",
    "        self.feature_data = {}\n",
    "        for idx in range(len(data['audio_name'])):\n",
    "            self.feature_data[str(data['audio_name'][idx].decode(\"utf-8\"))] = { 'feature': data['feature'][idx] }\n",
    "        # print(self.feature_data)\n",
    "        # print(self.data_frame.iloc[0,:])\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        file_name = str(self.data_frame.iloc[index, 0].split('/')[1]).split('.')[0]\n",
    "        label = 0#torch.tensor(self.data_frame.iloc[index, 1])\n",
    "        inp = self.feature_data[file_name]['feature']\n",
    "\n",
    "        return (inp, label, self.data_frame.iloc[index, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import code\n",
    "import os, torch, sys\n",
    "import torch\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from skimage.util import random_noise\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "epsilon = 0.0000000001\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "def submit(model_path, data=(hparams.submit_csv, hparams.submit_file), plot_auc='submit', plot_path=hparams.result_dir+'valid', best_thresh=None):\n",
    "\n",
    "    test_dataset = AudioData(data_csv=data[0], data_file=data[1], ds_type='submit',\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                        ]))\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=hparams.batch_size,\n",
    "                            shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "    discriminator = Discriminator().to(hparams.gpu_device)\n",
    "    if hparams.cuda:\n",
    "        discriminator = nn.DataParallel(discriminator, device_ids=hparams.device_ids)\n",
    "    checkpoint = torch.load(model_path, map_location=hparams.gpu_device)\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "\n",
    "    discriminator = discriminator.eval()\n",
    "    # print('Model loaded')\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if hparams.cuda else torch.FloatTensor\n",
    "\n",
    "    print('Testing model on {0} examples. '.format(len(test_dataset)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_logits_list = []\n",
    "        labels_list = []\n",
    "        img_names_list = []\n",
    "        # for _ in range(hparams.repeat_infer):\n",
    "        for (inp, labels, img_names) in tqdm(test_loader):\n",
    "            inp = Variable(inp.float(), requires_grad=False)\n",
    "            labels = Variable(labels.long(), requires_grad=False)\n",
    "\n",
    "            inp = inp.to(hparams.gpu_device)\n",
    "            labels = labels.to(hparams.gpu_device)\n",
    "\n",
    "            inp = inp.view(-1, 1, 640, 64)\n",
    "            inp = torch.cat([inp]*3, dim=1)\n",
    "\n",
    "            pred_logits = discriminator(inp)\n",
    "\n",
    "            pred_logits_list.append(pred_logits)\n",
    "            labels_list.append(labels)\n",
    "            img_names_list.append(img_names)\n",
    "\n",
    "        pred_logits = torch.cat(pred_logits_list, dim=0)\n",
    "        labels = torch.cat(labels_list, dim=0)\n",
    "        img_names = torch.cat(img_names_list, dim=0)\n",
    "        \n",
    "        pred_labels = pred_logits.max(1)[1]\n",
    "\n",
    "        \n",
    "        data_frame = pd.DataFrame({'Id': img_names, 'Scene_label': pred_labels})\n",
    "\n",
    "    print('Predictions saved to csv file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-leaderboard/evaluation_setup/test.csv' does not exist: b'/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-leaderboard/evaluation_setup/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f56b7165e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model/multiclass-CNN/model.best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-bb52ea86a817>\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(model_path, data, plot_auc, plot_path, best_thresh)\u001b[0m\n\u001b[1;32m     31\u001b[0m     test_dataset = AudioData(data_csv=data[0], data_file=data[1], ds_type='submit',\n\u001b[1;32m     32\u001b[0m                         transform=transforms.Compose([\n\u001b[0;32m---> 33\u001b[0;31m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                         ]))\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-afe165b71cdd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_csv, data_file, transform, input_shape, pre_process, ds_type)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-leaderboard/evaluation_setup/test.csv' does not exist: b'/data1/amey/ASR-data/TAU-urban-acoustic-scenes-2019-leaderboard/evaluation_setup/test.csv'"
     ]
    }
   ],
   "source": [
    "submit('../model/multiclass-CNN/model.best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../input/task1abasedensenet/model.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
